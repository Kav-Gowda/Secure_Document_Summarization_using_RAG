{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7_HqYvu78HGt"
      },
      "outputs": [],
      "source": [
        "# --- Environment Setup ---\n",
        "# Install required packages\n",
        "%%capture\n",
        "!pip install ibm-watsonx-ai==0.2.6 \\\n",
        "             langchain==0.1.16 \\\n",
        "             langchain-ibm==0.1.4 \\\n",
        "             transformers==4.41.2 \\\n",
        "             huggingface-hub==0.23.4 \\\n",
        "             sentence-transformers==2.5.1 \\\n",
        "             chromadb \\\n",
        "             wget==3.2 \\\n",
        "             --upgrade torch --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports and Warning Suppression ---\n",
        "\n",
        "# Suppress unwanted warnings\n",
        "import warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LangChain and document processing\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# IBM watsonx.ai and LangChain integration\n",
        "from ibm_watsonx_ai.foundation_models import Model\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
        "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "\n",
        "# Utilities\n",
        "import wget\n"
      ],
      "metadata": {
        "id": "OICCGWs09Fhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Private Document ---\n",
        "\n",
        "# Example placeholder for a private text document.\n",
        "# In actual use, replace 'data/private_document.txt' with your own file.\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "file_path = \"data/private_document.txt\"  # Local path (document not uploaded for confidentiality)\n",
        "loader = TextLoader(file_path)\n",
        "\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} document(s) for processing.\")\n"
      ],
      "metadata": {
        "id": "fHsMpfjY9lM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (Optional) Inspect Document Content ---\n",
        "\n",
        "# In practice, you might inspect the file before processing.\n",
        "# Skipping actual content display here to preserve data privacy.\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    contents = file.read()\n",
        "\n",
        "print(f\"Document loaded successfully. Total length: {len(contents)} characters.\")\n"
      ],
      "metadata": {
        "id": "5PJYlnrW-DE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Split Document into Chunks ---\n",
        "\n",
        "# Load and split the document into manageable text chunks\n",
        "loader = TextLoader(file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split into chunks for embedding and retrieval\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Document split into {len(texts)} chunks.\")\n"
      ],
      "metadata": {
        "id": "y7GzKvRy-LJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create Embeddings and Vector Store ---\n",
        "\n",
        "# Generate embeddings for each text chunk and store them using ChromaDB\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "vector_store = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "print(\"Documents embedded and stored in ChromaDB successfully.\")\n"
      ],
      "metadata": {
        "id": "CGBzGYm0-L-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Select LLM Model ---\n",
        "\n",
        "# Choose the foundation model from IBM watsonx.ai\n",
        "model_id = \"ibm/granite-3-2b-instruct\"\n",
        "\n",
        "print(f\"Selected model: {model_id}\")\n"
      ],
      "metadata": {
        "id": "prG0kjsu-WvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configure Model Generation Parameters ---\n",
        "\n",
        "parameters = {\n",
        "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,     # Deterministic output\n",
        "    GenParams.MIN_NEW_TOKENS: 130,                         # Minimum length of generated response\n",
        "    GenParams.MAX_NEW_TOKENS: 256,                         # Maximum length of generated response\n",
        "    GenParams.TEMPERATURE: 0.5                              # Controls creativity/randomness\n",
        "}\n",
        "\n",
        "print(\"Model generation parameters configured.\")\n"
      ],
      "metadata": {
        "id": "eJGN4peZ-f80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IBM watsonx.ai Credentials and Project Configuration ---\n",
        "\n",
        "# NOTE: API credentials are intentionally omitted for security and privacy.\n",
        "# To run this notebook, configure your IBM Cloud credentials securely.\n",
        "\n",
        "credentials = {\n",
        "    \"url\": \"your_watsonx_instance_url\",\n",
        "    \"api_key\": \"your_ibm_watsonx_api_key\"  # Replace with your own API key (kept private)\n",
        "}\n",
        "\n",
        "project_id = \"your_project_id\"\n",
        "\n",
        "print(\"Credentials and project configuration initialized (sensitive info hidden).\")\n"
      ],
      "metadata": {
        "id": "nglSc6Qu-m8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Initialize the watsonx.ai Model ---\n",
        "\n",
        "model = Model(\n",
        "    model_id=model_id,\n",
        "    params=parameters,\n",
        "    credentials=credentials,\n",
        "    project_id=project_id\n",
        ")\n",
        "\n",
        "print(\"watsonx.ai model initialized successfully.\")\n"
      ],
      "metadata": {
        "id": "LzJrH-2J_E2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Integrate watsonx.ai Model with LangChain ---\n",
        "\n",
        "llm = WatsonxLLM(model=model)\n",
        "print(\"LangChain LLM wrapper initialized successfully.\")\n"
      ],
      "metadata": {
        "id": "oPmWm6Q-_Kxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build RetrievalQA Chain and Query the Model ---\n",
        "\n",
        "# Create a retrieval-based QA chain using the LangChain + watsonx.ai LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=False\n",
        ")\n",
        "\n",
        "# Example query\n",
        "query = \"What is the company's lunch policy?\"\n",
        "response = qa.invoke(query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nResponse:\\n\", response[\"result\"])\n"
      ],
      "metadata": {
        "id": "jN-Y7dSg_g8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Summarize the Entire Document ---\n",
        "\n",
        "# You can reuse the same QA chain for summarization-style queries\n",
        "summary_query = \"Can you summarize the document for me?\"\n",
        "summary_response = qa.invoke(summary_query)\n",
        "\n",
        "print(\"Summary Query:\", summary_query)\n",
        "print(\"\\nSummary:\\n\", summary_response[\"result\"])\n"
      ],
      "metadata": {
        "id": "TvbsiW5w_3dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Custom Prompt Template ---\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Use the information from the provided context to answer the question below.\n",
        "If the answer cannot be found in the context, respond with \"I don't know.\"\n",
        "Do not attempt to fabricate an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "print(\"Custom prompt template defined successfully.\")\n"
      ],
      "metadata": {
        "id": "kNpscf5t_4-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Query with Custom Prompt Template ---\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        "    return_source_documents=False\n",
        ")\n",
        "\n",
        "custom_query = \"Can I eat in company vehicles?\"\n",
        "custom_response = qa.invoke(custom_query)\n",
        "\n",
        "print(\"Query:\", custom_query)\n",
        "print(\"\\nResponse:\\n\", custom_response[\"result\"])\n"
      ],
      "metadata": {
        "id": "2uCKre__BIjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build Conversational Retrieval Chain (with Memory) ---\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize conversation memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Create a conversational RAG chain\n",
        "conversational_qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    memory=memory,\n",
        "    get_chat_history=lambda h: h,\n",
        "    return_source_documents=False\n",
        ")\n",
        "\n",
        "# Example conversation\n",
        "query = \"What is the mobile policy?\"\n",
        "result = conversational_qa.invoke({\"question\": query, \"chat_history\": []})\n",
        "\n",
        "print(\"User Query:\", query)\n",
        "print(\"\\nAssistant Response:\\n\", result[\"answer\"])\n"
      ],
      "metadata": {
        "id": "xgu3OR9xBdlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Continue the Conversation ---\n",
        "\n",
        "# Follow-up query that uses prior context from memory\n",
        "follow_up_query = \"Can you list the key points from it?\"\n",
        "follow_up_result = conversational_qa.invoke({\"question\": follow_up_query, \"chat_history\": []})\n",
        "\n",
        "print(\"Follow-up Query:\", follow_up_query)\n",
        "print(\"\\nAssistant Response:\\n\", follow_up_result[\"answer\"])\n"
      ],
      "metadata": {
        "id": "D9xqrogUCBvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Wrap-up: Build the Intelligent Retrieval Agent\n",
        "\n",
        "This final section defines an interactive **Document Q&A Agent** powered by Retrieval-Augmented Generation (RAG), LangChain, and IBM watsonx.ai.\n",
        "The agent can retrieve information from your private documents, maintain conversation memory, and answer follow-up questions contextually â€” like a custom internal chatbot for document understanding.\n",
        "\n",
        "Once initialized, simply type your queries below to interact with the agent.\n"
      ],
      "metadata": {
        "id": "B31UmXOBC4qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Interactive Q&A Chatbot ---\n",
        "\n",
        "def start_chatbot():\n",
        "    \"\"\"\n",
        "    A simple interactive chatbot powered by watsonx.ai, LangChain, and RAG.\n",
        "    It retrieves answers from private documents and remembers previous context.\n",
        "    \"\"\"\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    conversational_qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vector_store.as_retriever(),\n",
        "        memory=memory,\n",
        "        get_chat_history=lambda h: h,\n",
        "        return_source_documents=False\n",
        "    )\n",
        "\n",
        "    print(\"Private Document Q&A Chatbot (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Question: \").strip()\n",
        "        if query.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Answer: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        result = conversational_qa.invoke({\"question\": query, \"chat_history\": []})\n",
        "        print(\"Answer:\", result[\"answer\"])\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Run the chatbot\n",
        "start_chatbot()\n"
      ],
      "metadata": {
        "id": "utgOcbsoDxAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}